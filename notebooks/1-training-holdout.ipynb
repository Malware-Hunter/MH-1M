{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as dataframe\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# import shap\n",
    "\n",
    "import xgboost\n",
    "print(xgboost.__version__)\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "!export PYTHONPATH=\"/home/hendrio/amex-1M/:$PYTHONPATH\"\n",
    "!export PYTHONPATH=\"/home/hendrio/amex-1M/src/:$PYTHONPATH\"\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/hendrio/amex-1M/')\n",
    "sys.path.append('/home/hendrio/amex-1M/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/hendrio/amex-1M/\"\n",
    "os.listdir(base_dir)\n",
    "\n",
    "paths={\n",
    "    'data': join(base_dir, 'data'),\n",
    "    'src': join(base_dir, 'data', 'src'),\n",
    "    'processed': join(base_dir, 'data', 'processed'),\n",
    "    'datasets': join(base_dir, 'datasets1'),\n",
    "    'results': join(base_dir,'results'),\n",
    "    'images': join(base_dir,'images'),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_amex_1m(path, file_name='amex-1M_binary-dataset-[intents-permissions-apicalls].npz'):\n",
    "    data = np.load(join(path, file_name), allow_pickle=True)\n",
    "    metadata = dataframe(data['metadata'], columns=data['metadata_columns'])\n",
    "    \n",
    "    columns_names = data['column_names']\n",
    "    sha256 = data['sha256']\n",
    "\n",
    "    labels_ohe = OneHotEncoder().fit_transform(np.expand_dims(metadata['CLASS'].values, axis=1)).toarray()\n",
    "\n",
    "    print(data['data'].shape, labels_ohe.shape)\n",
    "\n",
    "    print(metadata['CLASS'].value_counts())\n",
    "    return data['data'], labels_ohe, metadata, columns_names, sha256\n",
    "\n",
    "\n",
    "def create_class(df, threshold):\n",
    "    return np.asarray([1 if i>=threshold else 0 for i in df ])\n",
    "\n",
    "\n",
    "# # Split the data to get the indices\n",
    "# index_train, index_test = train_test_split(\n",
    "#     range(len(data)), \n",
    "#     test_size=0.30, \n",
    "#     random_state=0, \n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "def create_datasets(data, metadata):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split( \n",
    "        data, metadata, \n",
    "        test_size=0.30, \n",
    "        random_state=0, \n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    encoder_binary = OneHotEncoder().fit(np.expand_dims(metadata['CLASS'].values, axis=1))\n",
    "    \n",
    "    data_dict = {\n",
    "        'X_train': X_train,\n",
    "        'y_train_metadata': y_train,\n",
    "        'y_train_ohe': encoder_binary.transform(np.expand_dims(y_train['CLASS'].values, axis=1)).toarray(),\n",
    "        'X_test': X_test,\n",
    "        'y_test_metadata': y_test,\n",
    "        'y_test_ohe': encoder_binary.transform(np.expand_dims(y_test['CLASS'].values, axis=1)).toarray(),\n",
    "        'classes_names': ['Benign', 'Malware']\n",
    "    }\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'amex-1M-[intents-permissions-opcodes-apicalls]-chi2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340515, 12409) (1340515, 2)\n",
      "CLASS\n",
      "0    1221421\n",
      "1     119094\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data, labels_ohe, metadata, columns_names, sha256 = load_amex_1m(\n",
    "    path=paths['processed'], \n",
    "    file_name=f'{dataset_name}.npz'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(938360, 12409) (402155, 12409) (938360, 11) (402155, 11)\n"
     ]
    }
   ],
   "source": [
    "data_dict = create_datasets(data, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import trainer\n",
    "def pipeline_train_models(\n",
    "    data_dict, models_list, dataset_name='amex-1M', sufix_title='original', path_save=join(base_dir, 'models'), feature_list=None):\n",
    "\n",
    "    # for dataset_name in datasets.keys():\n",
    "    #     print(f'Dataset name: {dataset_name}')\n",
    "    for model_name in models_list:\n",
    "        print()\n",
    "        print()\n",
    "        print(f'=========================================================================')\n",
    "        print(f'===================== Model name: {model_name} ==========================')\n",
    "        print(f'=========================================================================')\n",
    "        \n",
    "        experiment_name = f'{dataset_name}-{model_name}-{sufix_title}'\n",
    "        print('\\t',experiment_name)\n",
    "        experiment_path = join(path_save, experiment_name)\n",
    "        \n",
    "        if os.path.exists(experiment_path):\n",
    "            print(f\"\\tExperiment already exists. \")\n",
    "            print(f'\\t============================================================================================')\n",
    "            print(f'\\t====================== Skipping {experiment_name} =====================')\n",
    "            print(f'\\t============================================================================================')\n",
    "            continue\n",
    "        else:\n",
    "            os.makedirs(experiment_path)\n",
    "\n",
    "        X_train, X_test = [], []\n",
    "        if feature_list is not None:\n",
    "            X_train = data_dict['X_train'][feature_list]\n",
    "            X_test = data_dict['X_test'][feature_list]\n",
    "        else:\n",
    "            X_train = data_dict['X_train']\n",
    "            X_test = data_dict['X_test']\n",
    "        print(f'\\tDataset shape: Train {X_train.shape}, Test {X_test.shape}')\n",
    "\n",
    "        print(f\"\\tTraining {model_name}\")\n",
    "        model=[]\n",
    "        extension='.joblib'\n",
    "        if model_name=='xgboost':\n",
    "            model, time_duration = trainer.train( \n",
    "                X=X_train , y=data_dict['y_train_ohe'], model_name=model_name)\n",
    "            extension='.joblib'\n",
    "        elif model_name=='randomforest':\n",
    "            model, time_duration = trainer.train( \n",
    "                X=X_train, y=data_dict['y_train_ohe'], model_name=model_name)\n",
    "            extension='.joblib'\n",
    "        elif model_name=='svm':\n",
    "            model, time_duration = trainer.train( \n",
    "                X=X_train, y=data_dict['y_train_ohe'], model_name=model_name)\n",
    "            extension='.joblib'\n",
    "        # elif model_name=='knn':\n",
    "        #     model, time_duration = trainer.train( \n",
    "        #         X=X_train, y=data_dict['y_train_ohe'], model_name=model_name)\n",
    "        #     extension='.joblib'\n",
    "        # elif model_name=='fcn':\n",
    "        #     model, time_duration = trainer.build_deep_model(\n",
    "        #         X=X_train, y=data_dict['y_train_ohe'], \n",
    "        #         num_classes=len(data_dict['classes_names']), \n",
    "        #         model_name=experiment_name, \n",
    "        #         path_experiment=experiment_path)\n",
    "        #     extension='.h5'\n",
    "\n",
    "        #     model = tf.keras.models.load_model(join(experiment_path, f'{experiment_name}_loss.keras'))\n",
    "        else:\n",
    "            raise Exception (f'Model Not Found on pipeline_train_models()')\n",
    "            os.remove(experiment_path)\n",
    "            return []\n",
    "\n",
    "        trainer.save_model(\n",
    "            model=model, \n",
    "            directory=experiment_path, \n",
    "            filename= experiment_name,\n",
    "            # extension=extension\n",
    "            )\n",
    "        \n",
    "        with open(join(experiment_path, f'{experiment_name}-time.txt'), 'w') as file:\n",
    "            file.write(f'{time_duration:.4f}')\n",
    "        print(f\"\\tEvaluating {model_name}\")\n",
    "        \n",
    "        pred, cm, report = trainer.eval(\n",
    "            X=X_test, \n",
    "            # y_true=np.argmax(y_test, axis=1), \n",
    "            y_true=data_dict['y_test_ohe'], \n",
    "            model=model,\n",
    "            class_labels=data_dict['classes_names'],\n",
    "            dataset_name=dataset_name,\n",
    "            model_name=model_name,\n",
    "            path_save=experiment_path\n",
    "        )\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        print('---------------------------------------------------------------------------------------------------------------')\n",
    "        print('')\n",
    "        print('')\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amex-1M-[intents-permissions-opcodes-apicalls]-chi2\n"
     ]
    }
   ],
   "source": [
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=========================================================================\n",
      "===================== Model name: xgboost ==========================\n",
      "=========================================================================\n",
      "\t amex-1M_binary-dataset-[intents-permissions]-xgboost-houdout\n",
      "\tDataset shape: Train (938360, 12409), Test (402155, 12409)\n",
      "\tTraining xgboost\n"
     ]
    }
   ],
   "source": [
    "pipeline_train_models(\n",
    "    data_dict=data_dict, \n",
    "    models_list=['xgboost'], #'knn', 'svm'\n",
    "    dataset_name='amex-1M_binary-dataset-[intents-permissions]', \n",
    "    sufix_title='houdout', \n",
    "    path_save=join(base_dir, 'results', 'models'),\n",
    "    feature_list = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
